{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/20204892/miniconda/envs/ly/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,GATConv\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "torch.cuda.set_device(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据读取\n",
    "\n",
    "def read(data_file):\n",
    "    proteins=open(f\"data/{data_file}\",\"r\").readlines()\n",
    "    res=[]\n",
    "    for i in range(0,len(proteins),2):\n",
    "        splited=proteins[i].split(\"|\")\n",
    "        name=proteins[i]\n",
    "        label=int(splited[1])\n",
    "        res.append((name,label))\n",
    "    return res\n",
    "\n",
    "\n",
    "data_dict={}\n",
    "\n",
    "proteins=open(\"PP/protein.dictionary.tsv\",\"r\").read().split(\"\\n\")\n",
    "proteins=[i.split(\"\\t\") for i in proteins]\n",
    "\n",
    "for name,seq in proteins:\n",
    "\n",
    "    vec=torch.from_numpy(np.load(f\"PP/feature/{name}.npy\")).float()[1:-1]\n",
    "    edge_matrix=torch.from_numpy(np.load(f\"PP/map/{name}.npy\")).float()\n",
    "    row, col = np.where((edge_matrix >= 0.5) & (np.eye(edge_matrix.shape[0]) == 0))\n",
    "    edge = [row.tolist(), col.tolist()]\n",
    "    edge_index=torch.from_numpy(np.array(edge)).long()\n",
    "    data=Data(x=vec,edge_index=edge_index,edge_matrix=edge_matrix)\n",
    "    data_dict[name]=data\n",
    "\n",
    "input_train_data=[]\n",
    "input_test_data=[]\n",
    "\n",
    "trains=open(\"PP/train_cmap.actions.tsv\",\"r\").read().split(\"\\n\")\n",
    "tests=open(\"PP/test_cmap.actions.tsv\",\"r\").read().split(\"\\n\")\n",
    "for i in trains:\n",
    "    items=i.split(\"\\t\")\n",
    "\n",
    "    items[2]=int(items[2])\n",
    "    input_train_data.append(items)\n",
    "\n",
    "for i in tests:\n",
    "    items=i.split(\"\\t\")\n",
    "    items[2]=int(items[2])\n",
    "    input_test_data.append(items)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \"\"\"\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v,  mask=None):\n",
    "        u = torch.bmm(q, k.transpose(1, 2)) # 1.Matmul\n",
    "        u = u / self.scale # 2.Scale\n",
    "\n",
    "        if mask is not None:\n",
    "            u = u.masked_fill(mask, -np.inf) # 3.Mask\n",
    "        \n",
    "        #print(u.shape,edge_matrix.shape)\n",
    "        #print(u)\n",
    "        #u[0]=u[1]=edge_matrix\n",
    "        attn = self.softmax(u) # 4.Softmax\n",
    "\n",
    "        output = torch.bmm(attn, v) # 5.Output\n",
    "\n",
    "        return output\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.fc_q = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_k = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_v = nn.Linear(d_v_, n_head * d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "\n",
    "        self.fc_o = nn.Linear(n_head * d_v, d_o)\n",
    "\n",
    "    def forward(self, q, k, v,  mask=None):\n",
    "\n",
    "        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
    "\n",
    "        batch, n_q, d_q_ = q.size()\n",
    "        batch, n_k, d_k_ = k.size()\n",
    "        batch, n_v, d_v_ = v.size()\n",
    "\n",
    "        q = self.fc_q(q) # 1.单头变多头\n",
    "        k = self.fc_k(k)\n",
    "        v = self.fc_v(v)\n",
    "        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)\n",
    "        k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)\n",
    "        v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "        output = self.attention(q, k, v,  mask=mask) # 2.当成单头注意力求输出\n",
    "\n",
    "        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1) # 3.Concat\n",
    "        output = self.fc_o(output) # 4.仿射变换得到最终输出\n",
    "\n",
    "        return output\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(n_head=4, d_k_=d, d_v_=d, d_k=d, d_v=d, d_o=d)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, q,k,v):\n",
    "        # 注意：实际情况中可能还会有一些其他的子层和残差连接\n",
    "        attn_output = self.self_attn(q,k,v)\n",
    "        v = v + attn_output\n",
    "        v = self.norm(v)\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, mean_squared_error, r2_score, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def testwithtrain(model):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(input_test_data)):\n",
    "            protrainx, protrainy, label = input_test_data[idx]\n",
    "            protrainx, protrainy = data_dict[protrainx], data_dict[protrainy]\n",
    "            protrainx, protrainy = protrainx.cuda(), protrainy.cuda()\n",
    "            out = model(protrainx, protrainy)\n",
    "            true_labels.append(np.array(label))\n",
    "            predicted_probs.append(out.cpu().numpy())\n",
    "\n",
    "    true_labels = np.array(true_labels).flatten()\n",
    "    predicted_probs = np.array(predicted_probs).flatten()\n",
    "\n",
    "    predicted_labels = (predicted_probs >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate confusion matrix components TN, FP, FN, TP\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
    "\n",
    "    # Calculate Accuracy (ACC)\n",
    "    acc = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Calculate Precision (Pre)\n",
    "    pre = precision_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Calculate Sensitivity (Sen) or Recall\n",
    "    sen = recall_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Calculate Specificity (Spe)\n",
    "    spe = tn / (tn + fp)\n",
    "\n",
    "    # Calculate F1 Score (F1)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Calculate Matthew Correlation Coefficient (MCC)\n",
    "    mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
    "\n",
    "    # Calculate Area Under the ROC Curve (AUC)\n",
    "    auc = roc_auc_score(true_labels, predicted_probs)\n",
    "\n",
    "    print(f'{acc * 100:.1f}', end=\"&\")\n",
    "    #print(f'{pre * 100:.1f}', end=\"&\")\n",
    "    #print(f'{sen * 100:.1f}', end=\"&\")\n",
    "    #print(f'{spe * 100:.1f}', end=\"&\")\n",
    "    print(f'{f1 * 100:.1f}', end=\"&\")\n",
    "    print(f'{mcc * 100:.1f}', end=\"&\")\n",
    "    print(f'{auc * 100:.1f}')\n",
    "\n",
    "\n",
    "\n",
    "    return acc, pre, sen, spe, f1, mcc, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256,0.0005,8\n"
     ]
    }
   ],
   "source": [
    "num_node_features=1280\n",
    "Hidden_feature,lr,batch_size=256,0.0005,8\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, Hidden_feature)\n",
    "        self.conv2 = GATConv(Hidden_feature, Hidden_feature)\n",
    "        self.attention = ScaledDotProductAttention(scale=np.power(Hidden_feature, 0.5))\n",
    "        self.liner1=nn.Linear(num_node_features, Hidden_feature)\n",
    "        self.cnn1 = nn.Conv1d(num_node_features, Hidden_feature, 3, stride=1, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(num_node_features, Hidden_feature, 5, stride=1, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(num_node_features, Hidden_feature, 7, stride=1, padding=3)\n",
    "        self.mha = TransformerLayer(Hidden_feature)\n",
    "        self.liner2=nn.Linear(Hidden_feature, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index,edge_matrix = data.x, data.edge_index,data.edge_matrix\n",
    "        value=F.relu(self.liner1(x)).unsqueeze(0)\n",
    "        key1=F.relu(self.cnn1(x.transpose(0, 1)).transpose(0, 1))\n",
    "        key2=F.relu(self.cnn2(x.transpose(0, 1)).transpose(0, 1))\n",
    "        key3=F.relu(self.cnn3(x.transpose(0, 1)).transpose(0, 1))\n",
    "\n",
    "        key=key1+key2+key3\n",
    "        key=key.unsqueeze(0)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        query = F.relu(self.conv2(x, edge_index))\n",
    "        query=query.unsqueeze(0)\n",
    "        x = self.mha(query,key,value)\n",
    "        x= x.squeeze(0)\n",
    "        x=torch.mean(x, dim=0)\n",
    "        return x\n",
    "\n",
    "class PPI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPI, self).__init__()\n",
    "        self.net1=GCN()\n",
    "        self.liner1=nn.Linear(Hidden_feature*2, Hidden_feature)\n",
    "        self.liner2=nn.Linear(Hidden_feature, Hidden_feature)\n",
    "        self.liner3=nn.Linear(Hidden_feature, 1)\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        x=self.net1(x)\n",
    "        y=self.net1(y)\n",
    "        x=torch.cat([x,y])\n",
    "        x=F.relu(self.liner1(x))\n",
    "        x=F.relu(self.liner2(x))\n",
    "        x=self.liner3(x)\n",
    "        x=torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "import copy\n",
    "train_data=copy.deepcopy(input_train_data)\n",
    "print(f\"{Hidden_feature},{lr},{batch_size}\")\n",
    "model = PPI().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCELoss()\n",
    "import random\n",
    "max_acc=0\n",
    "best_model=None\n",
    "models=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :\n",
      "51.8&54.7&20.7&82.8&30.1&4.6&54.7\n",
      "1 :\n",
      "67.4&63.2&83.5&51.4&71.9&36.8&72.2\n",
      "2 :\n",
      "72.0&70.5&75.6&68.4&73.0&44.1&78.2\n",
      "3 :\n",
      "80.3&79.3&81.9&78.6&80.6&60.5&87.0\n",
      "4 :\n",
      "87.6&88.3&86.6&88.6&87.5&75.2&94.6\n",
      "5 :\n",
      "89.4&89.9&88.7&90.1&89.3&78.8&95.8\n",
      "6 :\n",
      "92.6&93.4&91.7&93.6&92.6&85.3&97.8\n",
      "7 :\n",
      "94.2&94.8&93.6&94.9&94.2&88.5&98.5\n",
      "8 :\n",
      "93.9&95.8&91.9&96.0&93.8&87.9&98.4\n",
      "9 :\n",
      "96.3&95.4&97.3&95.3&96.3&92.6&99.3\n",
      "10 :\n",
      "96.4&95.6&97.3&95.5&96.5&92.9&99.4\n",
      "11 :\n",
      "95.6&95.4&95.8&95.4&95.6&91.2&99.1\n",
      "12 :\n",
      "95.4&96.3&94.5&96.3&95.4&90.8&99.1\n",
      "13 :\n",
      "96.6&97.2&95.9&97.2&96.5&93.1&99.4\n",
      "14 :\n",
      "96.2&96.9&95.4&97.0&96.2&92.4&99.4\n",
      "15 :\n",
      "96.7&96.4&97.0&96.4&96.7&93.4&99.3\n",
      "16 :\n",
      "97.1&96.9&97.3&96.9&97.1&94.2&99.5\n",
      "17 :\n",
      "95.8&96.2&95.4&96.2&95.8&91.7&99.2\n",
      "18 :\n",
      "97.1&97.6&96.6&97.6&97.1&94.2&99.3\n",
      "19 :\n",
      "96.5&96.2&96.8&96.2&96.5&92.9&99.6\n",
      "20 :\n",
      "97.4&97.6&97.2&97.6&97.4&94.8&99.6\n",
      "21 :\n",
      "96.9&96.7&97.1&96.7&96.9&93.8&99.3\n",
      "22 :\n",
      "95.8&96.7&94.8&96.8&95.8&91.6&98.9\n",
      "23 :\n",
      "96.1&98.0&94.2&98.0&96.0&92.3&99.2\n",
      "24 :\n",
      "97.4&97.5&97.2&97.5&97.4&94.7&99.7\n",
      "25 :\n",
      "96.4&97.5&95.3&97.6&96.4&92.9&99.3\n",
      "26 :\n",
      "96.7&97.6&95.8&97.7&96.7&93.5&99.2\n",
      "27 :\n",
      "96.6&97.6&95.5&97.7&96.6&93.2&99.1\n",
      "28 :\n",
      "97.5&97.2&97.9&97.1&97.6&95.1&99.6\n",
      "29 :\n",
      "97.1&97.8&96.3&97.9&97.1&94.2&99.5\n",
      "30 :\n",
      "96.6&96.5&96.8&96.5&96.7&93.3&99.3\n",
      "31 :\n",
      "96.4&97.3&95.5&97.3&96.4&92.9&99.0\n",
      "32 :\n",
      "97.5&97.8&97.3&97.8&97.5&95.1&99.4\n",
      "33 :\n",
      "96.7&97.1&96.3&97.1&96.7&93.5&99.1\n",
      "34 :\n",
      "97.7&98.4&97.0&98.4&97.7&95.4&99.6\n",
      "35 :\n",
      "96.9&97.9&95.8&97.9&96.8&93.8&99.4\n",
      "36 :\n",
      "97.3&97.4&97.1&97.4&97.3&94.5&99.1\n",
      "37 :\n",
      "95.5&95.1&95.9&95.1&95.5&91.0&99.0\n",
      "38 :\n",
      "97.8&98.3&97.2&98.3&97.8&95.5&99.5\n",
      "39 :\n",
      "98.2&98.1&98.2&98.1&98.2&96.3&99.6\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "for epochs in range(40):\n",
    "    optimizer.zero_grad()\n",
    "    losses=0\n",
    "    index=0\n",
    "    random.shuffle(train_data)\n",
    "    model.train()\n",
    "    for idx in range(len(train_data)):\n",
    "        protrainx,protrainy,label=train_data[idx]\n",
    "        protrainx,protrainy=data_dict[protrainx],data_dict[protrainy]\n",
    "        protrainx,protrainy=protrainx.cuda(),protrainy.cuda()\n",
    "        out = model(protrainx,protrainy)\n",
    "\n",
    "        label=torch.tensor(label).float().unsqueeze(0).unsqueeze(0)\n",
    "        label=label.cuda()\n",
    "        loss = criterion(out.unsqueeze(0), label)\n",
    "        loss.backward()\n",
    "        losses+=loss.item()\n",
    "        if index==batch_size:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            index=0\n",
    "\n",
    "        index+=1\n",
    "    print(len(models),\":\",end=\"\")\n",
    "    res_test=testwithtrain(model)\n",
    "    models.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc, pre, sen, spe, f1, mcc, auc\n",
    "98.2&98.2&96.3&99.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.2&98.1&98.2&98.1&98.2&96.3&99.6\n"
     ]
    }
   ],
   "source": [
    "res_test=testwithtrain(models[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(models[-1].cpu(), 'model_98.2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
